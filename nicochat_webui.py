import gradio as gr
import requests
import json

from ChatLib.prompt_gen import PromptGenerator, CharacterCard, UserCard  # å¼•å…¥PromptGeneratoråŠç›¸å…³ç±»
from ChatLib.ollama_request import OllamaLLMRequest
from ChatLib.lm_studio_request import LMStudioLLMRequest
from ChatLib.siliconflow_request import SilliconFlowLLMRequest

from ChatLib.voice_request import get_voice, get_voice_stream
import time


# è‡ªå®šä¹‰ CSSï¼Œç”¨æ¥æ¨¡æ‹Ÿå¼¹çª—å’Œå…¨å±é®ç½©æ•ˆæœ
custom_css = """
#setting-overlay {
    position: fixed;
    top: 0;
    left: 0;
    width: 100%;
    height: 100%;
    background: rgba(0,0,0,0.5);
    z-index: 500;
}
#setting-window {
    position: fixed;
    top: 30%;
    left: 40%;
    transform: translate(-50%, -50%);
    padding: 20px;
    z-index: 1000;
    border: 1px solid rgb(68, 68, 68);
    border-radius: 30px;
    margin: 10%;
    width: 80%;
    max-height: 80vh;
    overflow-y: auto;
}
@media (prefers-color-scheme: dark) {
    #setting-window {
        background: var(--block-border-color); /* æ·±è‰²æ¨¡å¼ä¸‹çš„èƒŒæ™¯é¢œè‰² */
    }
}
@media (prefers-color-scheme: light) {
    #setting-window {
        background: var(--block-border-color); /* æµ…è‰²æ¨¡å¼ä¸‹çš„èƒŒæ™¯é¢œè‰² */
    }
}
#setting-params{
    max-height: 50vh;
    display: list-item;
    overflow: overlay;
}
#chatbot {
    min-height: 50vh;
}
#func_btn{
    max-width: 50px;
    border-radius: 20px;
    font-size: 20px;
    padding-left: 0px;
    padding-right: 0px;
    min-width: 20px;
}
#func_btn_block{
    align-items: center;
}
#sending_text {
    min-width: 70%;
}
/*
#audio_blk {
    scale: 1;
}
*/
/* æ‰‹æœºç«¯çš„æŒ‰é’®æ ·å¼ */
@media screen and (max-width: 768px) {
    .gr-row { 
        flex-wrap: nowrap !important;  /* å¼ºåˆ¶å•è¡Œæ’åˆ— */
        overflow-x: auto !important;     /* å…è®¸æ¨ªå‘æ»šåŠ¨ */
    }
    .gr-row > .gr-button { 
        min-width: 30px !important;      /* å‹ç¼©æŒ‰é’®æœ€å°å®½åº¦ */
    }
}
.footer {
    display: none;
}
"""

# Get API_URL and API_KEY from user_setting.json
with open("./user_setting.json", "r", encoding="utf-8") as f:
    setting_json = json.load(f)
    DEFAULT_SERVER  = setting_json["DEFAULT_SERVER"]
    SERVER_LIST     = setting_json["SERVER_LIST"]
    REQUEST_DATA    = setting_json["REQUEST_DATA"]
    VOICE_SERVER    = setting_json["VOICE_SERVER"]

    print(SERVER_LIST)
    
    for server in SERVER_LIST:
        if server["SERVER"] == DEFAULT_SERVER:    config = server

    SERVER  = config["SERVER"]
    API_URL = config["API_URL"]
    API_KEY = config["API_KEY"]
    MODEL   = config["MODEL"]

    CHARACTER_NAME  = setting_json["CHARACTER_NAME"]
    USER_NAME       = setting_json["USER_NAME"]

def get_formatted_chat(chat_history):
    messages = []
    for message in chat_history:
        if message["role"] == CHARACTER_NAME:
            messages.append({"role": "assistant", "content": message['content']})
        elif message["role"] == USER_NAME:
            messages.append({"role": "user", "content": message['content']})
    return messages

def get_formatted_llm_hostory(chat_history):
    messages = []
    for message in chat_history:
        if message["role"] == "assistant":
            messages.append({"role": CHARACTER_NAME, "content": message['content']})
            log_record("TransLatedOK!!!")
        elif message["role"] == "user":
            messages.append({"role": USER_NAME, "content": message['content']})
    return messages

def log_record(log):
    with open("output.log", "a", encoding="utf-8") as f:  # å°†è¾“å‡ºå†™å…¥æ–‡ä»¶
        f.write(f"{log}\n")
def get_list(api_url):
    response = requests.get(api_url + "api/tags")
    response_dict = json.loads(response.text)
    models = [model["name"] for model in response_dict["models"]]
    for idx, name in enumerate(models, start=1):
        print(f"{idx}. {name}")
    return models
def generate_llm_response(api_url, message, model, top_k, top_p, temperature):
    """
    è°ƒç”¨åç«¯ API ç”Ÿæˆå›å¤ï¼Œå¹¶è¿”å›æ›´æ–°åçš„å¯¹è¯å†å²ã€‚
    æ­¤å¤„ä¸ºåŒæ­¥è°ƒç”¨ï¼ŒAPI è¿”å› JSON æ ¼å¼ï¼š{ "message": {"content": "ç”Ÿæˆçš„å›å¤æ–‡æœ¬"} }
    """
    llm_request.api_url         = api_url
    llm_request.selected_model  = model
    llm_request.prompt_gen.json_diag_history.append({"role": USER_NAME, "content": message})
    formatted_chat = get_formatted_chat(llm_request.prompt_gen.json_diag_history)
    yield formatted_chat
    log_record("Chat History as follows:")
    log_record(llm_request.prompt_gen.json_diag_history)

    data                = REQUEST_DATA
    data["model"]       = model
    data["messages"]    = llm_request.prompt_gen.FillingPromptGen()

    log_record("Prompt as follows:")
    log_record(f"{data}\n")
    
    res = ""
    llm_request.prompt_gen.json_diag_history.append({"role": CHARACTER_NAME, "content": ""})
    result = ""
    for partial_response in llm_request.get_response(data, data["stopping_strings"], webui=True):
        res = partial_response  # ä¿ç•™æœ€åä¸€æ¬¡çš„å®Œæ•´ç»“æœ
        print(res.rstrip(), end='', flush=True)
        result += res

        # å¦‚æœè¿”å›ä¸­åŒ…å« <think æ ‡ç­¾ï¼Œåˆ™ç­‰å¾…å®Œæ•´ç»“æŸæ ‡ç­¾å‡ºç°åå¤„ç†
        if "<think" in result:
            if "</think>\n\n" in result:
                # å»é™¤ <think> ... </think> éƒ¨åˆ†ä¹‹å‰çš„å†…å®¹
                result = result.split("</think>")[1].strip()
        else:
            # å¦‚æœæ£€æµ‹åˆ°åœæ­¢å­—ç¬¦ä¸²ï¼Œåˆ™ç»“æŸç”Ÿæˆ
            if data["stopping_strings"] and any(stop_string in result for stop_string in data["stopping_strings"]):
                detected_stop_string = next(
                    (stop_string for stop_string in data["stopping_strings"] if stop_string in result),
                    None
                )
                print(f"æ£€æµ‹åˆ°åœæ­¢å­—ç¬¦ä¸² \"{detected_stop_string}\"ï¼Œåœæ­¢ç”Ÿæˆã€‚\n")
                result = result.replace(detected_stop_string, "")
                
                if "<think" in result:
                    if "</think>\n\n" in result:
                        # å»é™¤ <think> ... </think> éƒ¨åˆ†ä¹‹å‰çš„å†…å®¹
                        result = result.split("</think>")[1].strip()
                        
                llm_request.prompt_gen.json_diag_history[-1] = {"role": CHARACTER_NAME, "content": result.rstrip()}
        
                # æ ¼å¼åŒ–èŠå¤©å†å²ä¸ºæ¶ˆæ¯æ°”æ³¡
                formatted_chat = get_formatted_chat(llm_request.prompt_gen.json_diag_history)
                yield formatted_chat
                break
            
        # æ¯æ¬¡æ”¶åˆ°æ–°å†…å®¹åï¼Œéƒ½yieldå½“å‰ç´¯è®¡çš„ç»“æœ
        if "<think" in result:  llm_request.prompt_gen.json_diag_history[-1] = {"role": CHARACTER_NAME, "content": f"{CHARACTER_NAME}æ­£åœ¨åŠ¨è„‘ç­‹..."}
        else:                   llm_request.prompt_gen.json_diag_history[-1] = {"role": CHARACTER_NAME, "content": result.rstrip()}
        
        # æ ¼å¼åŒ–èŠå¤©å†å²ä¸ºæ¶ˆæ¯æ°”æ³¡
        formatted_chat = get_formatted_chat(llm_request.prompt_gen.json_diag_history)
        yield formatted_chat
def retry_last(api_url, history, model, top_k, top_p, temperature):
    """
    é‡è¯•æœ€åä¸€æ¬¡ç”Ÿæˆã€‚
    å¦‚æœå†å²è®°å½•ä¸­æœ€åä¸€æ¡ä¸ºç”¨æˆ·æ¶ˆæ¯ï¼ˆå³æ²¡æœ‰å¯¹åº”çš„å›å¤ï¼‰ï¼Œåˆ™é‡æ–°è°ƒç”¨ç”Ÿæˆå‡½æ•°ï¼›
    å¦‚æœæœ€åä¸€æ¡ä¸ºåŠ©æ‰‹å›å¤ï¼Œåˆ™å–å€’æ•°ç¬¬äºŒæ¡ç”¨æˆ·æ¶ˆæ¯é‡æ–°ç”Ÿæˆå›å¤ï¼Œå¹¶åˆ é™¤åŸæ¥çš„å›å¤ã€‚
    """
    print(llm_request.prompt_gen.json_diag_history)
    if not llm_request.prompt_gen.json_diag_history:
        return llm_request.prompt_gen.json_diag_history
    
    if len(llm_request.prompt_gen.json_diag_history)>1:
        # åˆ¤æ–­å†å²è®°å½•é•¿åº¦ï¼šè‹¥ä¸ºå¥‡æ•°ï¼Œæœ€åä¸€æ¡ä¸ºè§’è‰²æ¶ˆæ¯ï¼›è‹¥ä¸ºå¶æ•°ï¼Œæœ€åä¸€æ¡ä¸ºç”¨æˆ·å›å¤
        if len(llm_request.prompt_gen.json_diag_history) % 2 == 1:
            last_user_msg = llm_request.prompt_gen.json_diag_history[-2]["content"]
            # å»é™¤æœ€åæœªå›å¤çš„ç”¨æˆ·æ¶ˆæ¯
            llm_request.prompt_gen.json_diag_history = llm_request.prompt_gen.json_diag_history[:-2]
        else:
            last_user_msg = llm_request.prompt_gen.json_diag_history[-1]["content"]
            # å»é™¤æœ€åä¸€å¯¹ç”¨æˆ·æ¶ˆæ¯åŠå…¶å›å¤
            llm_request.prompt_gen.json_diag_history = llm_request.prompt_gen.json_diag_history[:-1]
    
    yield get_formatted_chat(llm_request.prompt_gen.json_diag_history)
    # é‡æ–°ç”Ÿæˆå›å¤
    for partial_response in generate_llm_response(api_url, last_user_msg, model, top_k, top_p, temperature):
        yield partial_response

def revoke_user_chat(api_url, history, model, top_k, top_p, temperature):
    """
    æ’¤å›ç”¨æˆ·çš„æœ€åä¸€æ¬¡å¯¹è¯
    """
    print(llm_request.prompt_gen.json_diag_history)
    if not llm_request.prompt_gen.json_diag_history:
        return llm_request.prompt_gen.json_diag_history
    if len(llm_request.prompt_gen.json_diag_history)>1:
        # åˆ¤æ–­å†å²è®°å½•é•¿åº¦ï¼šè‹¥ä¸ºå¥‡æ•°ï¼Œæœ€åä¸€æ¡ä¸ºè§’è‰²æ¶ˆæ¯ï¼›è‹¥ä¸ºå¶æ•°ï¼Œæœ€åä¸€æ¡ä¸ºç”¨æˆ·å›å¤
        if len(llm_request.prompt_gen.json_diag_history) % 2 == 1:
            # å»é™¤æœ€åæœªå›å¤çš„ç”¨æˆ·æ¶ˆæ¯
            llm_request.prompt_gen.json_diag_history = llm_request.prompt_gen.json_diag_history[:-2]
        else:
            # å»é™¤æœ€åä¸€å¯¹ç”¨æˆ·æ¶ˆæ¯åŠå…¶å›å¤
            llm_request.prompt_gen.json_diag_history = llm_request.prompt_gen.json_diag_history[:-1]
    
    yield get_formatted_chat(llm_request.prompt_gen.json_diag_history)

def chat_clean():
    llm_request.prompt_gen = PromptGenerator(CharacterCard(CHARACTER_NAME), UserCard(USER_NAME))
    llm_request.prompt_gen.json_diag_history = [{"role": CHARACTER_NAME, "content": llm_request.prompt_gen.CharacterCard.CharacterGreeting}]
    return get_formatted_chat(llm_request.prompt_gen.json_diag_history)

# å®šä¹‰æ‰“å¼€å’Œå…³é—­å¼¹çª—çš„å›è°ƒå‡½æ•°
def open_setting():
    # æ›´æ–° modal å’Œé®ç½©å±‚ä¸ºå¯è§
    return gr.update(visible=True), gr.update(visible=True)

def close_setting():
    # éšè— modal å’Œé®ç½©å±‚
    return gr.update(visible=False), gr.update(visible=False), gr.update(value=SERVER), gr.update(value=API_URL), gr.update(value=API_KEY)
def update_server(server_new):
    global SERVER_LIST
    global SERVER  
    global API_URL 
    global API_KEY 
    global MODEL   
    global CHARACTER_NAME
    global USER_NAME     
    global REQUEST_DATA
    global llm_request

    for server in SERVER_LIST:
        if server["SERVER"] == server_new:    config = server

    return gr.update(value=config["API_URL"]), gr.update(value=config["API_KEY"])

def save_restart_setting(server_new, api_url, api_key, model, character_name, user_name, chat_history):
    global SERVER_LIST
    global SERVER  
    global API_URL 
    global API_KEY 
    global MODEL   
    global CHARACTER_NAME
    global USER_NAME     
    global REQUEST_DATA
    global llm_request

    for server in SERVER_LIST:
        if server["SERVER"] == server_new:    config = server

    SERVER  = config["SERVER"]
    API_URL = config["API_URL"]
    API_KEY = config["API_KEY"]
    MODEL   = config["MODEL"]

    CHARACTER_NAME  = setting_json["CHARACTER_NAME"]
    USER_NAME       = setting_json["USER_NAME"]
    
    if SERVER=="SiliconFlow":   llm_request = SilliconFlowLLMRequest(API_URL, API_KEY)
    elif SERVER=="Ollama":      llm_request = OllamaLLMRequest(API_URL)
    elif SERVER=="LMStudio":    llm_request = LMStudioLLMRequest(API_URL)
    llm_request.prompt_gen = PromptGenerator(CharacterCard(CHARACTER_NAME), UserCard(USER_NAME))
    llm_request.prompt_gen.json_diag_history = get_formatted_llm_hostory(chat_history)

    return gr.update(visible=True), gr.update(visible=True), gr.update(choices=llm_request.get_list(), value=MODEL)

def load_voice(chat_history):
    if VOICE_SERVER!="":
        voice_path = get_voice(CHARACTER_NAME, chat_history[-1]["content"], voice_server=VOICE_SERVER)
        print(voice_path)
        return gr.update(value=voice_path, autoplay=True)

def load_voice_stream(chat_history):
    if VOICE_SERVER!="":
        for partial_response in get_voice_stream(CHARACTER_NAME, chat_history[-1]["content"], voice_server=VOICE_SERVER):
            print(f"WaveFilePath: {partial_response[0]} WaveTime: {partial_response[1]}s")
            with open(partial_response[0], "rb") as f:
                print(len(f.read()))
            yield gr.update(value=partial_response[0], autoplay=True)
            time.sleep(partial_response[1])

with gr.Blocks(css=custom_css, title="Nicoâ­ï¸Chat", theme=gr.themes.Soft()) as demo:
    gr.Markdown(f"# Nicoâ˜†Chat with {CHARACTER_NAME}")

    if SERVER=="SiliconFlow":   llm_request = SilliconFlowLLMRequest(API_URL, API_KEY)
    elif SERVER=="Ollama":      llm_request = OllamaLLMRequest(API_URL)
    elif SERVER=="LMStudio":    llm_request = LMStudioLLMRequest(API_URL)
    try:
        models = llm_request.get_list()
    except:
        raise Exception("Get ModelList Failed, API_URL or API_KEY is wrong")
    
    
    with gr.Row():
        model_select  = gr.Dropdown(label="Select Model", choices=models, value=MODEL, show_label=False, container=False)
    
    # ä½¿ç”¨ gr.Chatbot ç»„ä»¶ï¼Œtype å‚æ•°è®¾ç½®ä¸º "messages"ï¼Œä»¥é‡‡ç”¨ {'role':..., 'content':...} æ ¼å¼
    chatbot = gr.Chatbot(value=chat_clean(), type="messages", render_markdown=False, elem_id="chatbot")
    
    with gr.Row(variant="panel"):
        txt = gr.Textbox(placeholder="è¯·è¾“å…¥æ¶ˆæ¯", show_label=False, elem_id="sending_text", container=False, submit_btn=True)
        # send_btn = gr.Button("å‘é€", elem_id="send_btn")
    
    with gr.Column():
        with gr.Row(elem_id="func_btn_block"):
            voice_btn = gr.Button("ğŸ—£ï¸", elem_id="func_btn")
            revoke_btn = gr.Button("â†©ï¸", elem_id="func_btn")
            retry_btn = gr.Button("ğŸ”", elem_id="func_btn")
            clear_btn = gr.Button("ğŸ§¹", elem_id="func_btn")
            btn_settings = gr.Button("âš™", elem_id="func_btn")
        voice_wuhu = gr.Audio(waveform_options=gr.WaveformOptions(show_recording_waveform=False), show_label=False, show_download_button=False, show_share_button=False, scale=0.3, elem_id="audio_blk", visible=False)
    ########## Setting UI ##########
    # æ¨¡æ‹Ÿçš„é®ç½©å±‚ï¼Œåˆå§‹çŠ¶æ€ä¸ºéšè—
    setting_overlay = gr.Column(visible=False, elem_id="setting-overlay")
    
    # æ¨¡æ‹Ÿçš„å¼¹çª—å®¹å™¨ï¼Œåˆå§‹çŠ¶æ€ä¸ºéšè—
    setting_window = gr.Group(visible=False, elem_id="setting-window")
    
    with setting_window:
        gr.Markdown("### æ¨¡å‹å‚æ•°è®¾ç½®")

        with gr.Tab("æ¨¡å‹å‚æ•°è®¾ç½®"):
            with gr.Column(elem_id="setting-params"):
                server_input  = gr.Dropdown(label="æœåŠ¡å•†é€‰æ‹©", choices=[item['SERVER'] for item in SERVER_LIST], value=SERVER)
                api_url_input = gr.Textbox(label="API URL", value=API_URL)
                api_key_input = gr.Textbox(label="API KEY", value=API_KEY)
                top_k = gr.Slider(0, 100, step=1, label="top_k", value=40, info="å½±å“ç”Ÿæˆå¤šæ ·æ€§ï¼Œå€¼è¶Šå¤§ç­”æ¡ˆè¶Šå¤šæ ·")
                top_p = gr.Slider(0.0, 1.0, step=0.05, label="top_p", value=0.9, info="ä¸ top_k è”åˆæ§åˆ¶ç”Ÿæˆæ¦‚ç‡åˆ†å¸ƒ")
                temperature = gr.Slider(0.0, 2.0, step=0.1, label="temperature", value=0.4, info="æ¸©åº¦è¶Šé«˜å›ç­”è¶Šéšæœº")
                wuhu01 = gr.Slider(0.0, 2.0, step=0.1, label="wuhu01", value=0.4, info="æ¸©åº¦è¶Šé«˜å›ç­”è¶Šéšæœº")
                wuhu02 = gr.Slider(0.0, 2.0, step=0.1, label="wuhu02", value=0.4, info="æ¸©åº¦è¶Šé«˜å›ç­”è¶Šéšæœº")
                wuhu03 = gr.Slider(0.0, 2.0, step=0.1, label="wuhu03", value=0.4, info="æ¸©åº¦è¶Šé«˜å›ç­”è¶Šéšæœº")
                wuhu04 = gr.Slider(0.0, 2.0, step=0.1, label="wuhu04", value=0.4, info="æ¸©åº¦è¶Šé«˜å›ç­”è¶Šéšæœº")
        
        with gr.Tab("èŠœæ¹–"):
            with gr.Column(elem_id="setting-params"):
                CharacterSel    = gr.Dropdown(label="å¯¹è¯è§’è‰²é€‰æ‹©", choices=["NicoNya"], value="NicoNya")
                UserSel         = gr.Dropdown(label="ç”¨æˆ·è§’è‰²é€‰æ‹©", choices=["morenico"], value="morenico")
                param_01 = gr.Textbox(label="param_01", value="param_01")
                param_02 = gr.Textbox(label="param_02", value="param_02")
        with gr.Row():
            btn_save_setting    = gr.Button("ğŸ’¾", elem_id="func_btn")
            btn_close_setting   = gr.Button("âŒ", elem_id="func_btn")
    
    # å‘é€æŒ‰é’®ï¼šè°ƒç”¨ generate_llm_response å‡½æ•°ï¼Œæ›´æ–°èŠå¤©å†å²ï¼ˆchatbot çš„å€¼å³ä¸ºå¯¹è¯è®°å½•ï¼‰
    txt.submit(
        generate_llm_response,
        inputs=[api_url_input, txt, model_select, top_k, top_p, temperature],
        outputs=chatbot
    ).then(
        fn=load_voice_stream,
        inputs=[chatbot],
        outputs=voice_wuhu
    )

    txt.submit(
        lambda : "",
        inputs=None,
        outputs=txt
    )

    voice_btn.click(
        load_voice_stream,
        inputs=chatbot,
        outputs=voice_wuhu
    )
    
    # æ¸…ç©ºèŠå¤©ï¼šç›´æ¥å°†å¯¹è¯å†å²ç½®ä¸ºç©ºåˆ—è¡¨
    clear_btn.click(chat_clean, outputs=chatbot)

    revoke_btn.click(
        fn=revoke_user_chat,
        inputs=[api_url_input, chatbot, model_select, top_k, top_p, temperature],
        outputs=chatbot
    )
    
    # é‡è¯•æŒ‰é’®ï¼šé‡è¯•æœ€åä¸€æ¬¡ç”Ÿæˆ
    retry_btn.click(
        retry_last,
        inputs=[api_url_input, chatbot, model_select, top_k, top_p, temperature],
        outputs=chatbot
    ).then(
        fn=load_voice_stream,
        inputs=[chatbot],
        outputs=voice_wuhu
    )

    server_input.change(
        fn=update_server,
        inputs=server_input,
        outputs=[api_url_input, api_key_input]
    )
    
    btn_save_setting.click(
        fn=save_restart_setting,
        inputs=[server_input, api_url_input, api_key_input, model_select, CharacterSel, UserSel, chatbot],
        outputs=[setting_window, setting_overlay, model_select]
    )
    # ç‚¹å‡»â€œè®¾ç½®â€æŒ‰é’®æ—¶ï¼Œæ˜¾ç¤ºå¼¹çª—å’Œé®ç½©å±‚
    btn_settings.click(fn=open_setting, inputs=None, outputs=[setting_window, setting_overlay])
    # ç‚¹å‡»â€œå…³é—­è®¾ç½®â€æŒ‰é’®æ—¶ï¼Œéšè—å¼¹çª—å’Œé®ç½©å±‚
    btn_close_setting.click(
        fn=close_setting, 
        inputs=None, 
        outputs=[setting_window, setting_overlay, server_input, api_url_input, api_key_input]
    )
    
demo.launch(server_name="0.0.0.0")